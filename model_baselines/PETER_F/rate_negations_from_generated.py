"""
Module to let PETER rate nagations generated by
https://huggingface.co/dapang/yelp_pos2neg_lm_bart_large?text=the+hotel+may+be+under+renovation+but+the+suite+is+gorgeous
"""
import os
import math
import torch
import argparse
import joblib
import torch.nn as nn
import numpy as np
import pandas as pd
from module import PETER
from tqdm import tqdm
from peter_predictor import PeterPredictor
from utils import DataLoader
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

def dictorize(user, item, rating, seq, feature):
    out = {
        'userid':user,
        'itemid':item,
        'rating':rating,
        'feature':feature,
        'review':seq
    }
    return out

def main(args):
    print('loading...')
    predictor = PeterPredictor(
        'PETER/model/tripadvisorf_backup/model.pt',
        './data/TripAdvisor/reviews.pickle',
        './data/TripAdvisor/1/',
        20000
    )
    corpus = pd.read_csv(args.test_csv_dir).to_dict('records')

    # negation model
    model_name = 'dapang/yelp_pos2neg_lm_bart_large'
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    model.to('cuda' if torch.cuda.is_available() else 'cpu')

    def generate_batch_negation(examples):
        batch = tokenizer(examples, max_length=1024, padding=True, truncation=True, return_tensors="pt")
        out = model.generate(batch['input_ids'].to(model.device), num_beams=5)
        negative_examples = tokenizer.batch_decode(out, skip_special_tokens=True)
        return negative_examples

    print('rating...')
    rankings = []

    flip_counter = 0
    n_samples = 10000
    out_string = ""
    positive_samples = [i for i in corpus if i[args.labelfield_key] >= 4.0]
    err = 0
    for i,sample in tqdm(enumerate(positive_samples[:n_samples]), total=len(positive_samples)):
        out_string = out_string+'\n## '+str(i+1)+'\\'+str(n_samples)+' ##\n'
        try:
            user, item, rating, seq, feature = sample['user'], sample['item'], sample['rating'], \
                    sample[args.textfield_key], sample['feature']
            ppl = predictor.ppl_from_input(dictorize(user, item, rating, seq, feature))
            seq_hat = generate_batch_negation([seq])[0]
            ppl_hat = predictor.ppl_from_input(dictorize(user, item, rating, seq_hat, feature))
        except:
            err += 1
            print('err occured, total till now: ', err)
            continue
        
        out_string += 'original: '+ seq + '\n'
        out_string += str(ppl)+'\n'
        out_string += 'negation: '+ seq_hat + '\n'
        out_string += str(ppl_hat)+'\n'
        if ppl_hat < ppl:
            flip_counter += 1
    out_string = out_string +'done, inconsistent labels: '+str(flip_counter)+'\n'
    

    with open(args.output_dir,'w') as ofp:
        ofp.write(out_string)

    print('done, inconsistent labels: '+str(flip_counter))

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    # settings
    parser.add_argument('--test_csv_dir', type=str, default='input_test.csv', help='input directory.')
    parser.add_argument('--textfield_key', type=str, default='text_predicted', help='text field.')
    parser.add_argument('--labelfield_key', type=str, default='rating_predicted', help='label field.')
    parser.add_argument('--output_dir', type=str, default='dumps/rate_negations_from_generated.out', help='output directory.')

    args = parser.parse_args()

    print(vars(args))
    main(args)
    